# -*- coding: utf-8 -*-
"""data_process.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJWiROuOaH-eHcMYh6teiHgC1HlCrLWN

Module for data preprocessing
"""

from google.colab import drive

root_path = '/content/drive'
drive.mount(root_path)

import os

import numpy as np
import pandas as pd

import nltk
from nltk import word_tokenize
import re

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
stopword_list = list(stopwords.words('english'))

from nltk.stem import WordNetLemmatizer

import gensim
from gensim import corpora, models

class DataProcessing():
    def __init__(self):
        pass

    def NormalizeCorpus(self, data: pd.DataFrame, data_column = 'article', lowercase = True, remove_stop_words = True, remove_special_chars = True, stemming = False, lemmatization = True):
        articles = data.loc[0:,data_column].tolist()
        stopword_list = nltk.corpus.stopwords.words('english')
        if stemming:
            ps = nltk.porter.PorterStemmer()
        if lemmatization:
            lemmatizer = WordNetLemmatizer()
        if remove_special_chars:
            pattern = r'[^a-zA-Z\s]|\[|\]'

        normalized_corpus = []
        normalized_corpus_tokens = []
        for doc in articles:
            if remove_special_chars:
                special_char_pattern = re.compile(r'([{.(-)!}])')
                doc = special_char_pattern.sub(" \\1 ", doc)
                doc = re.sub(pattern, '', doc) 
            if stemming:
                doc = ' '.join([ps.stem(word) for word in doc.split()])
            if lemmatization: 
                doc = ' '.join([lemmatizer.lemmatize(word) for word in doc.split()])
            if lowercase:
                doc = doc.lower()

            tokens = word_tokenize(doc)
            tokens = [token.strip() for token in tokens]

            if remove_stop_words:  
                if lowercase:
                    tokens = [token for token in tokens if token not in stopword_list]
                else:
                    tokens = [token for token in tokens if token.lower() not in stopword_list]
            doc = ' '.join(tokens)
            normalized_corpus.append(doc)
            normalized_corpus_tokens.append(tokens)
        data['normalized_article'] = normalized_corpus
        data['tokens'] = normalized_corpus_tokens

        return data

